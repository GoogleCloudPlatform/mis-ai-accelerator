{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright 2023 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JIpyR7i-XtMy"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/mis-ai-accelerator/raw/main/notebook.ipynb\">\n",
        "      <img src=\"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRQBAwQEBQQFCQUFCRQNCw0UFBQUFBQUFBQUFBQUFBQUFBQUFBQTEBQUFBQUFBAUFBQUFBQUFBQVDw8SExISDxQSDv/AABEIACAAIAMBEQACEQEDEQH/xAAZAAEAAwEBAAAAAAAAAAAAAAAGBAUHCQP/xAAsEAABBAEDAgUCBwAAAAAAAAACAQMEBREGEiEAMQcTFCIjQXEVFlFhkaHR/8QAGAEAAwEBAAAAAAAAAAAAAAAAAwQFAgb/xAApEQABAwMDBAEEAwAAAAAAAAABAgMRAAQhEjFBE1FhcSIFMpGhFIGx/9oADAMBAAIRAxEAPwDqTazVrauZLQPMWOybuzON20VXH9dFaR1XEomJIFYcVoQVdq8Km6Yso8PJg3LkRQlem35IQJE5+2VxnojzCmlKx8QSJ9VhtwLA7kTFV+o9aV1GNpFGwrfxuHWO2qQJkwWPgDKea4uFUGtybVc2qic98Y6YtrF1/QsoV01LCNQTOTwNpVGQmc0F66ba1J1DWElUExgcneBO5jFT9N2pXunauyL026ZFakL6N/z2MmCF8bmE3jzwWEymFwmel7loMPrZE/EkZEHBjIzB7icbUZlzqtJcxkA4MjPY4kdjzUi1JQq5hCRCSMmqELiNqntXsS8D917dDay4me48/rmtr+w0UmWTqC9HYenet/L5Ph6cmHD3dkIXCTaTmeyl7F7r1WQ0nC1AaepGdQ/IGQPXy4FIKWYKUkzonEfonE+8UBvm7y3v7Cvs5gP6aleHToyq+VMhsTjlEe03DIc7EUFUVcT4ULroLc2zLKHWUw6m5EKCVlGkDAAO+chJ+ZFRnEXDzqmnlS0pmCklIVqO5JG2MEj4ztWpeHUFis8PtMQ4rflRo9XFaaBZAyNoC0KInmh7XOET3DwvdOF65X6k4p29fcWZJWonBGSTwcj0cjY1fs20tWzTaBACQBmcAd+ffNWF/GnSq0wr3GQfznZIDe24P1Av2X9el7dTaHAXQY8bjzRnQtSYRv5oXaPM6iqL2tksu100qd+CdWzGAnlRQL3MlxvTHCBlEz/PVppKrZ1p5B1J1hWokxuPu3jyd/8AKnOaX0LbUIOkiIE/139UUkWUTSLtBWwWHNR3LmmmahvSzlewL+3Aqrkp3lWgx7SbUlDnOF7pXS0u8Drzh6aA6V9TUqOcITgKM5ConjxSJWm26bSBrVoCdECeMqPA4ImK0/QdZc1Om48e8ehHNHszXMeVHjN8ILLafURRMIv+dctfu27z5XbA6e6jJJ5J8mrVqh1toJeInwIA8D1SHqdTdHNdaPDWVIUUXkiTAcbdYlbVXaomJKBYUVUDQdhIhIu0lwqLhUpWF4bJ3WRKcgj2CJ5EjcSDkZBGKUubf+QjSDBEEH0Z/B2PjaDmovh14eQvD+rfbaRp+xlvuSJc0W1EnCNwjQEypFsBC2iikuERMqqqqqX6l9Rc+oOAnCUgADtAAngSYkmBnsIAxaWiLVJjKiSSfZn8DYeKW9SKer//2Q==\" alt=\"Vertex AI logo\">\n",
        "      Run in Vertex AI Workbench\n",
        "    </a>\n",
        "    </td>\n",
        "    <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/mis-ai-accelerator/blob/main/notebook.ipynb\"><img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgBAMAAACBVGfHAAAALVBMVEVHcEz0mQP6rgD6rAD6rQD0mwPxjwX5qwD6qwDocArzlQP9uQDobwrobwrocAqx6FUsAAAAD3RSTlMAL4e95XEUn//k//+R/8OlcE7aAAAA0klEQVR4AcXQPwhBURQG8A/Jgt4+SCmzZDAKySiD2UBZlAkzyimLSVH2JGaZ3l5WRe97u8I+Ojf1kJ3f8nW7t3v+4Fd8iWKhBjQzxbQFoy8io3iwrDGGaoiR64tRArDYiWqKMVkBYa5HveygJZKvlve00eY5CV83KwPAv+ERBzqmkkhHwxxunELvRKAidLGkDSAkQ6goTyArAALPFsLk9wvvj7pGm+6rSkrjRsfrY7K1ECOPXqd7uneStjfLhsbpNe2BxuxtH0uSzsfGLvOrhX95AI3Oednh8+dnAAAAAElFTkSuQmCC\" alt=\"Colab logo\">\n",
        "    Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/mis-ai-accelerator/\"><img src=\"data:image/png;base64,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\" alt=\"GitHub logo\">\n",
        "    View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Cloud Medical Imaging ML Development Accelerators\n",
        "\n",
        "Once you have labeled datasets we will ingest them in Healthcare API, use BigQuery to filter the cohorts of information, retrieve the relevant labeled images, train using the CXR foundational model, and ultimately test our trained model on staged data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "We'll perform some configuration, initialize a few constants, and install some packages here. These will be used by the rest of the notebook.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details.\n",
        "- Code was tested with Python v3.10.6 in VSCode, Vertex AI Workbench, and Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# Consts for your GCP environment\n",
        "PROJECT_ID          = '[CHANGEME]' #@param {type: 'string'}\n",
        "LOCATION            = 'us-central1' #@param {type: 'string'}\n",
        "DATASET_ID          = f'dataset_{random.randint(1,1000)}'\n",
        "STORE_ID            = 'store_id' #@param {type: 'string'}\n",
        "DICOMWEB_HOST       = f'https://healthcare.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/datasets/{DATASET_ID}/dicomStores/{STORE_ID}/dicomWeb'\n",
        "BQ_TABLE_ID         = 'metadata' #@param {type: 'string'}\n",
        "BQ_TABLE            = f'{PROJECT_ID}.{DATASET_ID}.{BQ_TABLE_ID}'\n",
        "\n",
        "# Other Consts\n",
        "DIAGNOSIS           = 'PNEUMOTHORAX'\n",
        "STAGED_DIR          = './data/staged/'\n",
        "STAGED_DICOM_DIR    = './data/staged/inputs/'\n",
        "DICOM_DIR           = './data/inputs'\n",
        "EMBEDDINGS_DIR      = './data/outputs'\n",
        "MODEL_DIR           = './data/outputs/model'\n",
        "MIN_STUDIES         = 195\n",
        "\n",
        "# Remove if you don't want this level of logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Authenticate; will only run if you're in Colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth\n",
        "    # Authenticate user for access. There will be a popup asking you to sign in with your user and approve access.\n",
        "    auth.authenticate_user()\n",
        "\n",
        "# Some basic input validation\n",
        "if PROJECT_ID == '[CHANGEME]':\n",
        "  raise ValueError('Please provide your own value for PROJECT_ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Needed packages for BQ and DICOMweb\n",
        "pip install db-dtypes dicomweb-client[gcp]\n",
        "\n",
        "# Model training dependencies\n",
        "pip install scikit-learn tf-models-official>=2.13.0\n",
        "\n",
        "# Package for CXR foundation model wrapper\n",
        "CXR_COMMIT='6c284364e3883353ed737545e475a1016620fd7e' # tested version locked\n",
        "CXR_URL=https://github.com/Google-Health/imaging-research/archive/${CXR_COMMIT}.zip\n",
        "if ! test -d imaging-research-${CXR_COMMIT}; then\n",
        "    curl -L -o imaging-research-${CXR_COMMIT}.zip ${CXR_URL}\n",
        "    unzip -o imaging-research-${CXR_COMMIT}.zip\n",
        "    pip install imaging-research-${CXR_COMMIT}/cxr-foundation/\n",
        "    rm -rf imaging-research-${CXR_COMMIT}.zip\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GCP resource setup\n",
        "\n",
        "Use Terraform to set up a [Healthcare API DICOM store](https://cloud.google.com/healthcare-api/docs/how-tos/dicom#creating_a_dicom_store), [BigQuery table](https://cloud.google.com/bigquery/docs/tables), and [streaming](https://cloud.google.com/healthcare-api/docs/how-tos/dicom-bigquery-streaming) of metadata from Healthcare API DICOM store to a BigQuery table.\n",
        "\n",
        "**NOTE:** You must grant the role(s) bigquery.dataEditor role to your project's Cloud Healthcare Service Agent, in order to have metadata streamed from Healthcare API DICOM store to BigQuery."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Retrieve staged data and scripts\n",
        "if ! test -d data || ! test -d tf; then\n",
        "    gsutil -m cp -r gs://mis-ai-accelerator/* .\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# Retrieve terraform executable locally\n",
        "TF_URL=https://releases.hashicorp.com/terraform/1.6.1/terraform_1.6.1_linux_amd64.zip\n",
        "if ! test -f terraform; then\n",
        "    curl -so terraform.zip ${TF_URL}\n",
        "    unzip terraform.zip\n",
        "    rm -rf terraform.zip\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# Deploy terraform\n",
        "!./terraform -chdir=./tf init\n",
        "!./terraform -chdir=./tf plan -var=\"project_id={PROJECT_ID}\" -var=\"location={LOCATION}\" -var=\"dataset_id={DATASET_ID}\" -var=\"store_id={STORE_ID}\" -var=\"table_id={BQ_TABLE_ID}\" -out tf.plan \n",
        "!./terraform -chdir=./tf apply tf.plan\n",
        "# TODO[JK]: If this step fails, stop execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "STATUS_CODE=$(curl --write-out %{http_code} --silent --output /dev/null metadata)\n",
        "if [[ \"${STATUS_CODE}\" -eq 200 ]]; then\n",
        "    # Set MIS label on current GCE instance\n",
        "    VMNAME=$(curl -H Metadata-Flavor:Google metadata/computeMetadata/v1/instance/hostname | cut -d. -f1)\n",
        "    ZONE=$(curl -H Metadata-Flavor:Google metadata/computeMetadata/v1/instance/zone | cut -d/ -f4)\n",
        "    gcloud compute instances update ${VMNAME} --zone=${ZONE} --update-labels=goog-packaged-solution=medical-imaging-suite\n",
        "    echo Set label on ${VMNAME}\n",
        "else\n",
        "    echo Skipping label since we\\'re not inside a GCE instance.\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Healthcare API and BigQuery client setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dicomweb_client.api import DICOMwebClient\n",
        "from dicomweb_client.ext.gcp.session_utils import create_session_from_gcp_credentials\n",
        "from dicomweb_client.ext.gcp.uri import GoogleCloudHealthcareURL\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize our DICOMweb client to Healthcare API\n",
        "dicomweb_client = DICOMwebClient(\n",
        "    url=str(GoogleCloudHealthcareURL.from_string(DICOMWEB_HOST)),\n",
        "    session=create_session_from_gcp_credentials(),\n",
        ")\n",
        "\n",
        "# Initialize our BigQuery client (need project ID for Colab)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare DICOM Instances\n",
        "\n",
        "Start by sending all positive and negative Pneumothorax datasets to the Healthcare API DICOM endpoint. There should be 200 studies in all (100 positive and 100 negative).\n",
        "\n",
        "### [Option 1] Send bulk studies from PACS to the Healthcare API DICOM Store\n",
        "\n",
        "If you're sending data from PACS, then the way in which you ingest the data will depend on the particular product you're using. If unsure, please reach out to the vendor for support. If your PACS only supports DIMSE based send, then you'll likely benefit from setting up a [DICOM adapter](https://github.com/GoogleCloudPlatform/healthcare-dicom-dicomweb-adapter).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Option 2] Send bulk studies from local staged content to the Healthcare API DICOM Store\n",
        "\n",
        "If you prefer sending the local staged content, then you can leverage the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import pydicom\n",
        "\n",
        "datasets = list(\n",
        "    map(lambda file: pydicom.dcmread(file), glob.glob(STAGED_DICOM_DIR + \"*.dcm\"))\n",
        ")\n",
        "dicomweb_client.store_instances(datasets=datasets)\n",
        "\n",
        "# Send a minimum amount of studies to use on top of foundational model\n",
        "if len(datasets) < MIN_STUDIES:\n",
        "    raise ValueError(f\"Please send the minimum amount of studies: {MIN_STUDIES}\")\n",
        "\n",
        "print(\"Sent {0} DICOM instances\".format(len(datasets)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Map DICOM instances with labels, using BigQuery\n",
        "\n",
        "Execute a query against BQ to get the list of DICOM instances that are positive and negative for Pneumothorax, based on presence of KOS.\n",
        "\n",
        "**NOTE:** If you've just recently pushed a bunch of images to the Healthcare API DICOM Store, then it might take a few seconds to propagate the metadata to BigQuery.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#   DICOM types\n",
        "#   Key Object Selection Document, SOPClassUID=1.2.840.10008.5.1.4.1.1.88.59, Modality=KO\n",
        "#   Secondary Capture Image Storage, SOPClassUID=1.2.840.10008.5.1.4.1.1.7, Modality=DX\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT\n",
        "  StudyInstanceUID,\n",
        "  SeriesInstanceUID,\n",
        "  SOPInstanceUID,\n",
        "  CAST (CAST (StudyInstanceUID IN (\n",
        "      SELECT\n",
        "        DISTINCT StudyInstanceUID\n",
        "      FROM\n",
        "        `{}`\n",
        "      WHERE\n",
        "        SOPClassUID='1.2.840.10008.5.1.4.1.1.88.59'\n",
        "        AND Modality='KO') AS INT64) AS FLOAT64) AS {}\n",
        "FROM\n",
        "  `{}`\n",
        "WHERE\n",
        "  SOPClassUID='1.2.840.10008.5.1.4.1.1.7'\n",
        "  AND Modality='DX'\n",
        "\"\"\".format(\n",
        "    BQ_TABLE, DIAGNOSIS, BQ_TABLE\n",
        ")\n",
        "\n",
        "bq_job_config = bigquery.QueryJobConfig(use_query_cache=False)\n",
        "bq_df = bq_client.query(query, bq_job_config).to_dataframe()\n",
        "\n",
        "# Waiting for BQ to be populated with enough metadata\n",
        "if len(bq_df) < MIN_STUDIES:\n",
        "    raise ValueError(\n",
        "        f\"Re-run this cell until BQ has been populated the minimum amount of metadata: {len(bq_df)}/{MIN_STUDIES}\"\n",
        "    )\n",
        "\n",
        "# Paths to download DICOM files\n",
        "bq_df[\"dicom_file\"] = bq_df[\"SOPInstanceUID\"].apply(\n",
        "    lambda x: os.path.join(DICOM_DIR, x + \".dcm\")\n",
        ")\n",
        "# Paths for generated embeddings\n",
        "bq_df[\"embedding_file\"] = bq_df[\"dicom_file\"].apply(\n",
        "    lambda x: os.path.join(\n",
        "        EMBEDDINGS_DIR, os.path.basename(x).replace(\".dcm\", \".tfrecord\")\n",
        "    )\n",
        ")\n",
        "\n",
        "display(bq_df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieve mapped DICOM files locally for training\n",
        "\n",
        "Use the DataFrame constructed in the previous BigQuery cell and retrieve the files locally from the Healthcare API DICOM store. These files will be used to generate the embeddings used in the training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(DICOM_DIR):\n",
        "    os.makedirs(DICOM_DIR)\n",
        "\n",
        "for _, row in bq_df.iterrows():\n",
        "    instance = dicomweb_client.retrieve_instance(\n",
        "        study_instance_uid=row[\"StudyInstanceUID\"],\n",
        "        series_instance_uid=row[\"SeriesInstanceUID\"],\n",
        "        sop_instance_uid=row[\"SOPInstanceUID\"],\n",
        "    )\n",
        "    instance.save_as(row[\"dicom_file\"])\n",
        "\n",
        "if len(bq_df) < MIN_STUDIES:\n",
        "    raise ValueError(\n",
        "        f\"Please locally download the minimum amount of studies: {len(bq_df)}/{MIN_STUDIES}\"\n",
        "    )\n",
        "\n",
        "print(\"Downloaded {0} DICOM images\".format(len(bq_df)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqdct5F0VFPw"
      },
      "source": [
        "## Generate Embeddings\n",
        "\n",
        "**IMPORTANT:** You must have access to use the CXR Foundation API. See the project's [README](https://github.com/Google-Health/imaging-research/blob/master/cxr-foundation/README.md) for details.\n",
        "\n",
        "This will request the embeddings from the Vertex AI endpoint for the CXR foundational model and may take ~15 minutes depending on the load on the server and your connection speed. You may think of embeddings as compressed raster images, in a format efficient for model training.\n",
        "\n",
        "_There may be some warnings about \"Could not load dynamic library\" and or \"No project ID could be determined,\" but these can be safely ignored._\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8liPYf1JGdMO",
        "outputId": "c24fd68a-6fcd-4e7a-fa9f-fbae4b96531e"
      },
      "outputs": [],
      "source": [
        "from cxr_foundation.inference import generate_embeddings, InputFileType, OutputFileType\n",
        "\n",
        "if not os.path.exists(EMBEDDINGS_DIR):\n",
        "    os.makedirs(EMBEDDINGS_DIR)\n",
        "\n",
        "generate_embeddings(\n",
        "    input_files=bq_df[\"dicom_file\"].values,\n",
        "    output_dir=EMBEDDINGS_DIR,\n",
        "    input_type=InputFileType.DICOM,\n",
        "    output_type=OutputFileType.TFRECORD,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHZSZO_hDdl"
      },
      "source": [
        "### Prepare Data for Model Training\n",
        "\n",
        "Separate into training, validation, and testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Wr8tsaTLNpEH",
        "outputId": "dc6b0d7d-7e67-4149-dd86-2726626fd739"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_validate = train_test_split(bq_df, test_size=0.1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fVGYhxEkWBhs"
      },
      "source": [
        "## Train A Model\n",
        "\n",
        "Finally, we can train a model using the embeddings! With a simple feed-forward neural network, it should take < 5 minutes to train 100 epochs! No GPU required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_models as tfm\n",
        "from cxr_foundation import embeddings_data\n",
        "\n",
        "\n",
        "def create_model(\n",
        "    heads,\n",
        "    embeddings_size=1376,\n",
        "    learning_rate=0.1,\n",
        "    end_lr_factor=1.0,\n",
        "    dropout=0.0,\n",
        "    decay_steps=1000,\n",
        "    loss_weights=None,\n",
        "    hidden_layer_sizes=[512, 256],\n",
        "    weight_decay=0.0,\n",
        "    seed=None,\n",
        ") -> tf.keras.Model:\n",
        "    # Creates linear probe or multilayer perceptron using LARS + cosine decay.\n",
        "    inputs = tf.keras.Input(shape=(embeddings_size,))\n",
        "    hidden = inputs\n",
        "    # If no hidden_layer_sizes are provided, model will be a linear probe.\n",
        "    for size in hidden_layer_sizes:\n",
        "        hidden = tf.keras.layers.Dense(\n",
        "            size,\n",
        "            activation=\"relu\",\n",
        "            kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "            bias_regularizer=tf.keras.regularizers.l2(l2=weight_decay),\n",
        "        )(hidden)\n",
        "        hidden = tf.keras.layers.BatchNormalization()(hidden)\n",
        "        hidden = tf.keras.layers.Dropout(dropout, seed=seed)(hidden)\n",
        "    output = tf.keras.layers.Dense(\n",
        "        units=len(heads),\n",
        "        activation=\"sigmoid\",\n",
        "        kernel_initializer=tf.keras.initializers.HeUniform(seed=seed),\n",
        "    )(hidden)\n",
        "\n",
        "    outputs = {}\n",
        "    for i, head in enumerate(heads):\n",
        "        outputs[head] = tf.keras.layers.Lambda(\n",
        "            lambda x: x[..., i : i + 1], name=head.lower()\n",
        "        )(output)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    learning_rate_fn = tf.keras.experimental.CosineDecay(\n",
        "        tf.cast(learning_rate, tf.float32),\n",
        "        tf.cast(decay_steps, tf.float32),\n",
        "        alpha=tf.cast(end_lr_factor, tf.float32),\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tfm.optimization.lars.LARS(learning_rate=learning_rate_fn),\n",
        "        loss=dict([(head, \"binary_crossentropy\") for head in heads]),\n",
        "        loss_weights=loss_weights or dict([(head, 1.0) for head in heads]),\n",
        "        weighted_metrics=[\n",
        "            tf.keras.metrics.FalsePositives(),\n",
        "            tf.keras.metrics.FalseNegatives(),\n",
        "            tf.keras.metrics.TruePositives(),\n",
        "            tf.keras.metrics.TrueNegatives(),\n",
        "            tf.keras.metrics.AUC(),\n",
        "            tf.keras.metrics.AUC(curve=\"PR\", name=\"auc_pr\"),\n",
        "        ],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# Create training and validation Datasets\n",
        "training_data = embeddings_data.get_dataset(\n",
        "    filenames=df_train[\"embedding_file\"].values, labels=df_train[DIAGNOSIS].values\n",
        ")\n",
        "\n",
        "\n",
        "validation_data = embeddings_data.get_dataset(\n",
        "    filenames=df_validate[\"embedding_file\"].values, labels=df_validate[DIAGNOSIS].values\n",
        ")\n",
        "\n",
        "# Create and train the model\n",
        "model = create_model([DIAGNOSIS])\n",
        "\n",
        "model.fit(\n",
        "    x=training_data.batch(512).prefetch(tf.data.AUTOTUNE).cache(),\n",
        "    validation_data=validation_data.batch(1).cache(),\n",
        "    epochs=100,\n",
        ")\n",
        "\n",
        "# Summary after training is complete\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run prediction on local images\n",
        "\n",
        "Load some images that the model has not seen before and test them locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from cxr_foundation import constants\n",
        "from typing import Sequence\n",
        "from cxr_foundation.inference import (\n",
        "    create_example_from_image,\n",
        "    _embedding_from_service,\n",
        "    InputFileType,\n",
        ")\n",
        "\n",
        "\n",
        "# TODO[JK]: Promote this code to cxr-foundation\n",
        "def embedding_and_model_predict(\n",
        "    image_file: str, model: tf.keras.Model\n",
        ") -> Sequence[float]:\n",
        "    example = create_example_from_image(image_file, InputFileType.DICOM)\n",
        "    embedding = _embedding_from_service(\n",
        "        example,\n",
        "        constants._EMBEDDINGS_PROJECT_NAME,\n",
        "        constants._LOCATION,\n",
        "        constants._ENDPOINT_ID,\n",
        "    )\n",
        "    return model.predict(np.array(embedding[0]))\n",
        "\n",
        "\n",
        "for file in [STAGED_DIR + \"positive.dcm\", STAGED_DIR + \"negative.dcm\"]:\n",
        "    result = embedding_and_model_predict(file, model)\n",
        "    print(\"File: {0}, result: {1}\".format(file, result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Uncomment to destroy resources (default=OFF for Run All)\n",
        "# !./terraform -chdir=./tf destroy -auto-approve"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8LpEO7UrU9eS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3ac608b8f9188be2227ae82298dfd5de684cbdc4496f362d4b3b9040509447c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
